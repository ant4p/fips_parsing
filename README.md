### Парсер fips.ru ###

Для запуска - склонируйте проект командой git clone https://github.com/ant4p/fips_parsing.git <br/>
Создайте виртуальное окружение.<br/>
Установите в виртуальное окружение файл requirements.txt командой pip install -r requirements.txt<br/>
<br/>
Файл .env_example замените на .env c вашими данными: <br/>
-- не меняются --<br/>
BASE_URL="https://www.fips.ru/publication-web/classification/mpk" - основа для парсинга международной патентной классификации<br/>
EDITION='2025' - данные для GET запроса по году<br/>
-- меняются при необходимости --<br/>
FOLDER_WITH_JSON_PATH="json_files/" - папка в которой будут сохраняться json файлы, в корне проекта<br/>
OUTPUT_JSON_FILE="union.json" - название файла с конечным результатом<br/>
INPUT_MPK_FILENAME = "Подклассы МПК.xlsx" - файл с данными подклассов МПК<br/>
<br/>
Запустите проект в корневой папке командой python main.py<br/>
При текущих вводных данных, а именно поиск по 32ум подклассам МПК:<br/>
время работы парсера около 47,5-48 секунд: из них 32 секунды - <br/>
timeout между запросами, 15-16 секунд чистое время работы.<br/>
На выходе получаем папку с файлами json по каждому виду МПК,<br/>
а так-же общий union.json файл на чуть больше 38000 строк, который создаётся в корне проекта<br/>
Все из созданных json файлов имеют иерархическую структуру, и могут содержать<br/>
многоуровневые вложенные подгруппы<br/>
<br/>
<br/>
<br/>
Описание програмного решения:<br/>
Процесс парсинга сайта fips.ru декомпозирован на задачи:<br/>
1. Выделить модули кода согласно логике, чтобы можно было их легко заменять или переиспользовать.<br/>
Так в файлах:<br/>
parser_mpk - находится сам парсер<br/>
utils.py - находятся функции которые осуществляют обработку сырых дынных<br/>
script.py - находятся функции которые осуществляют обработку уже готовых данных полученных путём парсинга, при желании эти функции можно переписать на скрипты bash или powershell <br/>
main.py - основной файл проекта, точка входа, в котором весь процесс ETL данных собирается из модулей.
2. Получаем список МПК из файла, по которым будем осуществлять сбор информации<br/>
3. Используем парсер для получения сырых данных и сохранения их в датафрейме pandas проходясь по каждому элементу из полученного списка МПК<br/>
4. Сохраняем данные в json формате для каждого конкретного МПК в отдельной папке<br/>
5. Получаем список сохранённых json файлов и объединяем в один общий json файл.<br/>
6. Замеряем время работы всей сборки и выборочно проверяем полученные данные.<br/>
7. При желании проверяем полученные данные на интерактивной диаграме plotly или древовидной структуре tree.html файла.<br/>
<br/>
<br/>
Для того, чтобы наглядно проверить полученные данные в папке output присутствуют 2 html файла:<br/>
plotly.html - настроен на круговую иерархическую интерактивную диаграмму по файлу 'B63B.json' <br/>
соответственно вы можете воспользоваться им после парсинга<br/>
<br/>
<p align="center">
 <img width=auto height=1000 src="images/plotly_html_view.png" alt="coverage"/>
</p>
<br/>
<p align="center">
 <img width=auto height=1000 src="images/plotly_html_view_2.png" alt="coverage"/>
</p>
<br/>
tree.html - настроен на показ иерархической древовидной структура распарсеных данных<br/>
из объединённого файла union.json, который соответственно будет доступен тоже после парсинга<br/>
<br/>
<p align="center">
 <img width=auto height=1000 src="images/tree_html_view.png" alt="coverage"/>
</p>
<br/>
Обе html страницы будут корректно работать на localhost при использовании Live Server.<br/>
<br/>
Для сервиса использованы:<br/>
python3.12 - ЯП<br/>
python-dotenv переменные окружения .env<br/>
bs4 - для извлечения данных<br/>
pandas - обработка данных<br/>
openpyxl - обработка данных для .xlsx файлов<br/>
plotly - интерактивная диаграмма через cdn<br/>
os, time, requests, json - стандартные python библиотеки<br/>
live server - для локального просмотра html страниц
<br/>
